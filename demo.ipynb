{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8ab83-b3a4-44bb-91f4-4f51b5111d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import gradio as gr\n",
    "import argparse\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from detgpt.common.config import Config\n",
    "from detgpt.common.dist_utils import get_rank\n",
    "from detgpt.common.registry import registry\n",
    "from detgpt.conversation.conversation import Chat, Conversation, SeparatorStyle  # , CONV_VISION\n",
    "from GroundingDINO.groundingdino.models import build_model\n",
    "from GroundingDINO.groundingdino.util.slconfig import SLConfig\n",
    "from GroundingDINO.groundingdino.util.utils import clean_state_dict\n",
    "from GroundingDINO.groundingdino.util import box_ops\n",
    "from GroundingDINO.groundingdino.util.inference import annotate, load_image, predict\n",
    "import GroundingDINO.groundingdino.datasets.transforms as T\n",
    "from segment_anything import build_sam, SamPredictor \n",
    "from huggingface_hub import hf_hub_download\n",
    "from supervision.draw.color import Color, ColorPalette\n",
    "import re\n",
    "import sys\n",
    "sys.path.append(sys.path[0]+\"/tracker\")\n",
    "from tracker.base_tracker import BaseTracker\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8a8c9-f6b2-4dd9-86a9-86e248b628fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.cfg_path = \"configs/detgpt_tasktune_13b_coco.yaml\"  # replace with your actual path\n",
    "        self.system_path = None  # replace with your actual path if needed\n",
    "        self.dino_version = \"swinb\"\n",
    "        self.gpu_id = 0\n",
    "        self.options = None  # replace with your actual options if needed\n",
    "        self.disable_detector = True\n",
    "        self.enable_system = False\n",
    "\n",
    "args = Args()\n",
    "print('Initializing Chat')\n",
    "cfg = Config(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a9efe-2a0f-48e1-b838-ab2593687f4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cuda_llm = f\"cuda:0\"\n",
    "cuda_detector = f\"cuda:0\"\n",
    "cuda_sam = f\"cuda:0\"\n",
    "ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n",
    "ckpt_sam_path = \"output_models/sam_vit_h_4b8939.pth\"\n",
    "if args.dino_version == \"swinb\":\n",
    "    config_file = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinB_cfg.py\"\n",
    "    ckpt_filenmae = \"groundingdino_swinb_cogcoor.pth\"\n",
    "else:\n",
    "    config_file = \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "    ckpt_filenmae = \"groundingdino_swint_ogc.pth\"\n",
    "\n",
    "if args.system_path:\n",
    "    with open(args.system_path, 'r') as file:\n",
    "        system_message = file.read()\n",
    "        print(f\"system message: \\n {system_message}\")\n",
    "else:\n",
    "    system_message = \"You must strictly answer the question step by step:\\n\" \\\n",
    "                     \"Step-1. describe the given image in detail.\\n\" \\\n",
    "                     \"Step-2. find all the objects related to user input, and concisely explain why these objects meet the requirement.\\n\" \\\n",
    "                     \"Step-3. list out all related objects existing in the image strictly as follows: <Therefore the answer is: [object_names]>.\\n\" \\\n",
    "                     \"Complete all 3 steps as detailed as possible.\\n\" \\\n",
    "                     \"You must finish the answer with complete sentences.\"\n",
    "\n",
    "CONV_VISION = Conversation(\n",
    "    system=system_message,\n",
    "    roles=(\"Human\", \"Assistant\"),\n",
    "    messages=[],\n",
    "    offset=2,\n",
    "    sep_style=SeparatorStyle.SINGLE,\n",
    "    sep=\"###\",\n",
    ")\n",
    "\n",
    "\n",
    "def load_model_hf(model_config_path, repo_id, filename, device='cpu'):\n",
    "    model_args = SLConfig.fromfile(model_config_path)\n",
    "    model = build_model(model_args)\n",
    "\n",
    "    cache_file = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "    checkpoint = torch.load(cache_file, map_location='cpu')\n",
    "    log = model.load_state_dict(clean_state_dict(checkpoint['model']), strict=False)\n",
    "    print(\"Model loaded from {} \\n => {}\".format(cache_file, log))\n",
    "    _ = model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def image_transform_grounding(init_image):\n",
    "    transform = T.Compose([\n",
    "        T.RandomResize([800], max_size=1333),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image, _ = transform(init_image, None)  # 3, h, w\n",
    "    return init_image, image\n",
    "\n",
    "\n",
    "def image_transform_grounding_for_vis(init_image):\n",
    "    transform = T.Compose([\n",
    "        T.RandomResize([800], max_size=1333),\n",
    "    ])\n",
    "    image, _ = transform(init_image, None)  # 3, h, w\n",
    "    return image\n",
    "\n",
    "\n",
    "def print_format(message):\n",
    "    print(f\"*\" * 20)\n",
    "    print(f\"\\n{message}\\n\")\n",
    "\n",
    "\n",
    "def list_to_str(cat_list, sep=\". \"):\n",
    "    result = \"\"\n",
    "    for cat in cat_list:\n",
    "        result += cat\n",
    "        result += sep\n",
    "    return result[:-1]\n",
    "\n",
    "\n",
    "def run_grounding(input_image, llm_message_original, box_threshold, text_threshold):\n",
    "    init_image = input_image.convert(\"RGB\")\n",
    "    original_size = init_image.size\n",
    "    _, image_tensor = image_transform_grounding(init_image)\n",
    "    image_pil: Image = image_transform_grounding_for_vis(init_image)\n",
    "    response_message = llm_message_original[-1]\n",
    "    print_format(f\"From run grounding, oringinal response message {response_message}\")\n",
    "    pattern1 = r\"(?i)therefore,?\\s+the\\s+answer\\s+is:?[\\s\\[\\],]*(\\w+[\\s,]*)+([ ,]\\w+[\\s,]*)*\"\n",
    "    pattern2 = r\"(?i)therefore,?\\s+the\\s+target\\s+objects?\\s+are:?[\\s\\[\\],]*(\\w+[\\s,]*)+([ ,]\\w+[\\s,]*)*\"\n",
    "    # Use re.search() to find the match\n",
    "    match1 = re.search(pattern1, response_message)\n",
    "    match2 = re.search(pattern2, response_message)\n",
    "    # Extract the matched substring\n",
    "    if match1:\n",
    "        substr = match1.group(0)\n",
    "        # Remove the unnecessary characters\n",
    "        substr = re.sub(r\"(?i)therefore,?\\s+the\\s+answer\\s+is:?[\\s\\[\\],]*\", \"\", substr)\n",
    "        categories = re.sub(r\"[\\[\\]]\", \"\", substr)\n",
    "        cat_list = [c.strip() for c in categories.split(',')]\n",
    "        # remove duplicate\n",
    "        cat_list = list(set(cat_list))\n",
    "        categories = list_to_str(cat_list)\n",
    "        # Print the result\n",
    "        print_format(f\"Detected categores: {categories}\")\n",
    "    elif match2:\n",
    "        substr = match2.group(0)\n",
    "        # Remove the unnecessary characters\n",
    "        substr = re.sub(r\"(?i)therefore,?\\s+the\\s+target\\s+objects?\\s+are:?[\\s\\[\\],]*\", \"\", substr)\n",
    "        categories = re.sub(r\"[\\[\\]]\", \"\", substr)\n",
    "        cat_list = [c.strip() for c in categories.split(',')]\n",
    "        # remove duplicate\n",
    "        cat_list = list(set(cat_list))\n",
    "        categories = list_to_str(cat_list)\n",
    "        # Print the result\n",
    "        print_format(f\"Detected categores: {categories}\")\n",
    "    else:\n",
    "        print_format(\"No match found.\")\n",
    "        categories = \"\"\n",
    "    # run grounidng\n",
    "    # boxes, logits, phrases = predict(detector, image_tensor, categories, box_threshold, text_threshold, device=f\"cuda:{args.gpu_id[0]}\")\n",
    "    boxes, logits, phrases = predict(detector, image_tensor, categories, box_threshold, text_threshold,\n",
    "                                     device=cuda_detector)\n",
    "    print_format(f\"Detector predicted phrases {phrases}\")\n",
    "\n",
    "    annotated_frame = annotate(image_source=np.asarray(image_pil), boxes=boxes, logits=logits, phrases=phrases)\n",
    "    annotated_frame = annotated_frame[...,::-1]\n",
    "    segmented_frame_masks = run_segment(np.asarray(image_pil), sam_predictor, boxes=boxes)\n",
    "    annotated_frame_with_mask = draw_masks(segmented_frame_masks[0], annotated_frame)\n",
    "    image_with_box_sam = Image.fromarray(annotated_frame_with_mask)\n",
    "\n",
    "    return image_with_box_sam, f\"{categories}\"\n",
    "def run_segment(image, sam_model, boxes):\n",
    "    sam_model.set_image(image)\n",
    "    H, W, _ = image.shape\n",
    "    boxes_xyxy = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n",
    "\n",
    "    transformed_boxes = sam_model.transform.apply_boxes_torch(boxes_xyxy.to(cuda_sam), image.shape[:2])\n",
    "    masks, _, _ = sam_model.predict_torch(\n",
    "        point_coords = None,\n",
    "        point_labels = None,\n",
    "        boxes = transformed_boxes,\n",
    "        multimask_output = False,\n",
    "        )\n",
    "    return masks.cpu()\n",
    "  \n",
    "\n",
    "def draw_masks(masks, image):\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    colors = ColorPalette.default()\n",
    "    for idx, mask in enumerate(masks):\n",
    "        color = colors.by_idx(idx).as_rgb()\n",
    "        color = np.array([color[0]/255, color[1]/255, color[2]/255, 0.6])\n",
    "\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        \n",
    "        \n",
    "        mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "        annotated_frame_pil = Image.alpha_composite(annotated_frame_pil, mask_image_pil)\n",
    "    return np.array(annotated_frame_pil)\n",
    "def draw_mask(mask, image):\n",
    "    annotated_frame_pil = Image.fromarray(image).convert(\"RGBA\")\n",
    "    colors = ColorPalette.default()\n",
    "    idx = 0\n",
    "    color = colors.by_idx(idx).as_rgb()\n",
    "    color = np.array([color[0]/255, color[1]/255, color[2]/255, 0.6])\n",
    "\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "\n",
    "\n",
    "    mask_image_pil = Image.fromarray((mask_image.cpu().numpy() * 255).astype(np.uint8)).convert(\"RGBA\")\n",
    "    annotated_frame_pil = Image.alpha_composite(annotated_frame_pil, mask_image_pil)\n",
    "    return np.array(annotated_frame_pil)\n",
    "\n",
    "def setup_seeds(config):\n",
    "    seed = config.run_cfg.seed + get_rank()\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc258699-12a1-43bf-bb43-74b8dcfa42a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### sam 30384m\n",
    "detector = load_model_hf(config_file, ckpt_repo_id, ckpt_filenmae)\n",
    "detector = detector.to(cuda_detector)\n",
    "sam_predictor = SamPredictor(build_sam(checkpoint=ckpt_sam_path).to(cuda_sam))\n",
    "xmem = BaseTracker(\"./output_models/XMem-s012.pth\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487eaf79-a9b8-4a04-aa6e-f0787d5dd898",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "### language model 27830m\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = cuda_llm\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model_llm = model_cls.from_config(model_config).to(cuda_llm)\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.coco_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)\n",
    "chat = Chat(model_llm, vis_processor, device=cuda_llm)\n",
    "print_format('Initialization Finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ed112-5ab1-4f54-b392-164f9a9d8876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gpt(img, user_message):\n",
    "    user_prompt = \"\\nThis is the first frame of a video. You need to answer me and decide a object to track according to my question. End the answer by listing out the only target object to my question strictly as follows: <Therefore the answer is: [object_names]>. \\nMy question:\"\n",
    "    img_list = []\n",
    "    chat_state = CONV_VISION.copy()\n",
    "    chat.upload_img(img, chat_state, img_list)\n",
    "    chat.ask(user_message + user_prompt, chat_state)\n",
    "    num_beams = 5\n",
    "    temperature = 0\n",
    "    length_penalty = 0.5\n",
    "    do_sample = False\n",
    "\n",
    "\n",
    "\n",
    "    llm_message_old = chat.answer(conv=chat_state,\n",
    "                                    img_list=img_list,\n",
    "                                    num_beams=num_beams,\n",
    "                                    temperature=temperature,\n",
    "                                    length_penalty=length_penalty,\n",
    "                                    do_sample=do_sample,\n",
    "                                    max_new_tokens=300,\n",
    "                                    max_length=2000)[0]\n",
    "    return llm_message_old\n",
    "\n",
    "def get_categories(message):\n",
    "    pattern1 = r\"(?:Therefore, the answer is|Therefore the answer is).*\"\n",
    "    pattern2 = r\"(?:Therefore, the target objects are|Therefore the target objects are).*\"\n",
    "    llm_message = re.sub(pattern1, \"\", message)\n",
    "    llm_message = re.sub(pattern2, \"\", llm_message)\n",
    "    pattern1 = r\"(?i)therefore,?\\s+the\\s+answer\\s+is:?[\\s\\[\\],]*(\\w+[\\s,]*)+([ ,]\\w+[\\s,]*)*\"\n",
    "    pattern2 = r\"(?i)therefore,?\\s+the\\s+target\\s+objects?\\s+are:?[\\s\\[\\],]*(\\w+[\\s,]*)+([ ,]\\w+[\\s,]*)*\"\n",
    "    # Use re.search() to find the match\n",
    "    match1 = re.search(pattern1, message)\n",
    "    match2 = re.search(pattern2, message)\n",
    "    # Extract the matched substring\n",
    "    if match1:\n",
    "        substr = match1.group(0)\n",
    "        # Remove the unnecessary characters\n",
    "        substr = re.sub(r\"(?i)therefore,?\\s+the\\s+answer\\s+is:?[\\s\\[\\],]*\", \"\", substr)\n",
    "        categories = re.sub(r\"[\\[\\]]\", \"\", substr)\n",
    "        cat_list = [c.strip() for c in categories.split(',')]\n",
    "        # remove duplicate\n",
    "        cat_list = list(set(cat_list))\n",
    "        categories = list_to_str(cat_list)\n",
    "        # Print the result\n",
    "        print_format(f\"Detected categores: {categories}\")\n",
    "    elif match2:\n",
    "        substr = match2.group(0)\n",
    "        # Remove the unnecessary characters\n",
    "        substr = re.sub(r\"(?i)therefore,?\\s+the\\s+target\\s+objects?\\s+are:?[\\s\\[\\],]*\", \"\", substr)\n",
    "        categories = re.sub(r\"[\\[\\]]\", \"\", substr)\n",
    "        cat_list = [c.strip() for c in categories.split(',')]\n",
    "        # remove duplicate\n",
    "        cat_list = list(set(cat_list))\n",
    "        categories = list_to_str(cat_list)\n",
    "        # Print the result\n",
    "        print_format(f\"Detected categores: {categories}\")\n",
    "    else:\n",
    "        categories = \"\"\n",
    "    return categories\n",
    "\n",
    "def run_grounding(img,categories):\n",
    "    box_threshold, text_threshold = 0.25, 0.25\n",
    "    original_size = img.size\n",
    "    _, image_tensor = image_transform_grounding(img)\n",
    "    image_pil: Image = image_transform_grounding_for_vis(img)\n",
    " \n",
    "    \n",
    "    # run grounidng\n",
    "    # boxes, logits, phrases = predict(detector, image_tensor, categories, box_threshold, text_threshold, device=f\"cuda:{args.gpu_id[0]}\")\n",
    "    boxes, logits, phrases = predict(detector, image_tensor, categories, box_threshold, text_threshold,\n",
    "                                        device=cuda_detector)\n",
    "    print_format(f\"Detector predicted phrases {phrases}\")\n",
    "    # choose the one with highest score\n",
    "    annotated_frame = annotate(image_source=np.asarray(image_pil), boxes=boxes[logits.argmax()].reshape((1,-1)), logits=logits[logits.argmax()].reshape((1)), phrases=[phrases[logits.argmax()]])\n",
    "    annotated_frame = annotated_frame[...,::-1]\n",
    "    segmented_frame_masks = run_segment(np.asarray(image_pil), sam_predictor, boxes=boxes[logits.argmax()])\n",
    "    annotated_frame_with_mask = draw_mask(segmented_frame_masks[logits.argmax()], annotated_frame)\n",
    "    image_with_box_sam = Image.fromarray(annotated_frame_with_mask)\n",
    "    return segmented_frame_masks\n",
    "\n",
    "def semi_track(images, template_mask):\n",
    "    xmem.clear_memory()\n",
    "    masks = []\n",
    "    logits = []\n",
    "    painted_images = []\n",
    "    images = video_state[\"painted_images\"]\n",
    "    \n",
    "    # resize template_mask to images[0].shape\n",
    "    template_mask = cv2.resize(template_mask.astype(np.uint8), (images[0].shape[1], images[0].shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "    for i in tqdm(range(len(images)), desc=\"Tracking image\"):\n",
    "        if i ==0:           \n",
    "            mask, logit, painted_image = xmem.track(images[i], template_mask)\n",
    "            masks.append(mask)\n",
    "            logits.append(logit)\n",
    "            painted_images.append(painted_image)\n",
    "\n",
    "        else:\n",
    "            mask, logit, painted_image = xmem.track(images[i])\n",
    "            masks.append(mask)\n",
    "            logits.append(logit)\n",
    "            painted_images.append(painted_image)\n",
    "    return painted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36d004-e8b3-4322-8384-95d651732ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def track(images, user_message):\n",
    "    img = Image.fromarray(images[0]).convert('RGB')\n",
    "    llm_message_old = gpt(img, user_message)\n",
    "    response_message = llm_message_old\n",
    "    print_format(f\"From run grounding, oringinal response message {response_message}\")\n",
    "    categories = get_categories(response_message)\n",
    "    \n",
    "    segmented_frame_masks = run_grounding(img,categories)\n",
    "    template_mask = segmented_frame_masks[0][0].cpu().numpy()\n",
    "    painted_images = semi_track(images, template_mask)\n",
    "    return painted_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bffb520-b712-46f0-94fd-731ab36d7a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import argparse\n",
    "import gdown\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "video_state = gr.State(\n",
    "    {\n",
    "    \"user_name\": \"\",\n",
    "    \"video_name\": \"\",\n",
    "    \"origin_images\": None,\n",
    "    \"painted_images\": None,\n",
    "    \"masks\": None,\n",
    "    \"inpaint_masks\": None,\n",
    "    \"logits\": None,\n",
    "    \"select_frame_number\": 0,\n",
    "    \"fps\": 30\n",
    "    }\n",
    ")\n",
    "\n",
    "video_path = \"./examples/elon.mp4\"\n",
    "frames = []\n",
    "user_name = time.time()\n",
    "try:\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            current_memory_usage = psutil.virtual_memory().percent\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            if current_memory_usage > 90:\n",
    "                operation_log = [(\"Memory usage is too high (>90%). Stop the video extraction. Please reduce the video resolution or frame rate.\", \"Error\")]\n",
    "                print(\"Memory usage is too high (>90%). Please reduce the video resolution or frame rate.\")\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "except (OSError, TypeError, ValueError, KeyError, SyntaxError) as e:\n",
    "    print(\"read_frame_source:{} error. {}\\n\".format(video_path, str(e)))\n",
    "image_size = (frames[0].shape[0],frames[0].shape[1]) \n",
    "# initialize video_state\n",
    "video_state = {\n",
    "    \"user_name\": user_name,\n",
    "    \"video_name\": os.path.split(video_path)[-1],\n",
    "    \"origin_images\": frames,\n",
    "    \"painted_images\": frames.copy(),\n",
    "    \"masks\": [np.zeros((frames[0].shape[0],frames[0].shape[1]), np.uint8)]*len(frames),\n",
    "    \"logits\": [None]*len(frames),\n",
    "    \"select_frame_number\": 0,\n",
    "    \"fps\": fps\n",
    "    }\n",
    "video_info = \"Video Name: {}, FPS: {}, Total Frames: {}, Image Size:{}\".format(video_state[\"video_name\"], video_state[\"fps\"], len(frames), image_size)\n",
    "img = Image.fromarray(video_state[\"origin_images\"][video_state[\"select_frame_number\"]]).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6546da5-969d-4c41-8140-2412843d8a13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "following_frames = video_state[\"origin_images\"][video_state[\"select_frame_number\"]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73977c-f32b-4faf-b4f5-0a6aa312a143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "painted_images = track(following_frames, \"I want to track elon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77488ded-f22f-4e7e-8b68-402d45a2cf0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def generate_video_from_frames(frames, output_path, fps=30):\n",
    "    \"\"\"\n",
    "    Generates a video from a list of frames.\n",
    "    \n",
    "    Args:\n",
    "        frames (list of numpy arrays): The frames to include in the video.\n",
    "        output_path (str): The path to save the generated video.\n",
    "        fps (int, optional): The frame rate of the output video. Defaults to 30.\n",
    "    \"\"\"\n",
    "    frames = torch.from_numpy(np.asarray(frames))\n",
    "    if not os.path.exists(os.path.dirname(output_path)):\n",
    "        os.makedirs(os.path.dirname(output_path))\n",
    "    torchvision.io.write_video(output_path, frames, fps=fps, video_codec=\"libx264\")\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce00f3-1d2b-4ef9-bb41-b5c8152d154b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_video_from_frames(painted_images,\"./examples/elon_output.mp4\", fps=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
